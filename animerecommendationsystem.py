# -*- coding: utf-8 -*-
"""AnimeRecommendationSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wPwIBm--3k2F6WcsiRctymC10zxuEx7G

# Proyek Sistem Rekomendasi : Sistem rekomendasi Anime
- Nama : Andi Sadapotto
- Email : andi.sadapotto.m@gmail.com
- ID Dicoding : andi_sadapotto
"""

!pip install fake-useragent
!pip install scikit-surprise

"""## Import package & library"""

import seaborn as sns
import pandas as pd
import requests
import shutil
import numpy as np
import os
import zipfile
import tensorflow as tf


from PIL import Image, UnidentifiedImageError
from matplotlib import pyplot as plt
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split as surprise_train_test_split
from surprise import accuracy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

"""## Data Loading

### Download dan ekstrak dataset
Download dataset dari sistus penyedia kaggle kemudian mengekstrak dan menyimpan dataset kedalam folder dataset
"""

# Download  dataset
!curl -L -o archive.zip https://www.kaggle.com/api/v1/datasets/download/marlesson/myanimelist-dataset-animes-profiles-reviews

# Ekstrak  dataset
with zipfile.ZipFile('archive.zip', 'r') as zip_ref:
    zip_ref.extractall('dataset')  # Ekstrak ke folder 'dataset'

# Hapus file zipe
os.remove('archive.zip')

#Tampilkan list konten dari folder 'dataset'
!ls -l dataset

"""### Load dataset"""

df_anime = pd.read_csv('dataset/animes.csv')
df_review = pd.read_csv('dataset/reviews.csv')
df_user = pd.read_csv('dataset/profiles.csv')

df_review.head()

"""## Data Undestanding

Menampilkan jumlah baris dan kolom tiap dataset
"""

# Tampilkan jumlah data dan kolom untuk masing-masing dataset
print("Jumlah data dan kolom df_anime:")
print(df_anime.shape)
print("\nJumlah data dan kolom df_review:")
print(df_review.shape)
print("\nJumlah data dan kolom df_user:")
df_user.shape

"""Menampilkan list variabel tiap dataset"""

print("List variabel df_anime:")
print(df_anime.columns.tolist())
print("\nList variabel df_review:")
print(df_review.columns.tolist())
print("\nList variabel df_user:")
print(df_user.columns.tolist())

"""### Variabel Description

animes.csv memiliki 19311 baris dan 12 kolom dengan deskripsi sebagai berikut

Variable | Keterangan
------|------
uid| Kode unik untuk tiap anime
title| Judul dari anime
synopsis| Ringkasan atau ikhtisar dari sebuah anime
genre| List genre dari anime
aired| Tanggal tayang
episodes| Jumlah episode
members| Total member pada komunitas
popularity| Popularitas di situs MyAnimelist
ranked| Rangking/peringkat di situs MyAnimelist
score| Skor/rating di situs MyAnimelist
img_url|link thumbnail anime
link|link anime di MyAnimelist

reviews.csv memiliki 192112 baris dan 7 kolom dengan deskripsi sebagai berikut

Variable | Keterangan
------|------
uid| id unik untuk masing-masing review
profile| username dari pengguna yang memberikan review
anime_uid| anime uid yang di review
text| text review
score| overall skor review yang diberikan
scores |detail score yang diberikan
link| link detail review

profiles.csv memiliki 81727 baris dan 5 kolom dengan deskripsi sebagai berikut

Variable | Keterangan
------|------
profile | username unik untuk tiap pengguna
gender | jenis kelamin
birthday | tanggal lahir
favorites_anime | list anime favorit
link | link profil pengguna

### Assesing Data

Cek missing value
"""

df_anime.isnull().sum()

df_review.isnull().sum()

df_user.isnull().sum()

"""Cek data duplicate"""

df_anime.duplicated().sum()

df_review.duplicated().sum()

df_user.duplicated().sum()

"""### Exploratory Data Analysis (EDA) & Visualisasi Data

#### Dataset Anime

Deskripsi statistik dataset anime
"""

df_anime.info()

df_anime.describe(include='all')

"""Grouping berdasarkan genre"""

df_anime.groupby('genre').count().sort_values(by='uid', ascending=False)

# show data based on ranked
df_anime[['title','ranked']].sort_values(by='ranked', ascending=True)

df_anime.synopsis.mode()[0]

# Create the figure and axes
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Distribution of Anime Scores
sns.histplot(df_anime['score'].dropna(), kde=True, ax=axes[0, 0])
axes[0, 0].set_title('Distribution of Anime Scores')

# Plot 2: Top 10 Genres
top_genres = df_anime.groupby('genre').size().sort_values(ascending=False).head(10)
sns.barplot(x=top_genres.index, y=top_genres.values, ax=axes[0, 1])
axes[0, 1].set_title('Top 10 Most Genres')
axes[0,1].tick_params(axis='x', rotation=45) # Rotate x-axis labels for better readability


# Plot 3: Relationship between Score and Number of Episodes
sns.scatterplot(x='episodes', y='score', data=df_anime.dropna(subset=['episodes', 'score']), ax=axes[1, 0])
axes[1, 0].set_title('Score vs. Number of Episodes')


# Plot 4: Relationship between Score and Members
sns.regplot(x='members', y='score', data=df_anime.dropna(subset=['members', 'score']), ax=axes[1, 1])
axes[1, 1].set_title('Score vs. Members')

# Adjust layout to prevent overlapping titles and labels
plt.tight_layout()

# Show the plot
plt.show()



"""#### Dataset review"""

df_review.info()

df_review.describe(include='all')

df_review.groupby('scores').count().sort_values(by='uid', ascending=False)

df_review.groupby('profile').count().sort_values(by='uid', ascending=False)

# Create the figure and axes
fig, axes = plt.subplots(2, 1, figsize=(10, 6))

# Plot 1: Distribution of Review Scores
sns.histplot(df_review['score'].dropna(), kde=True, ax=axes[0])
axes[0].set_title('Distribution of Review Scores')

# Plot 2: Top 10 Reviewers
top_reviewers = df_review.groupby('profile').size().sort_values(ascending=False).head(10)
sns.barplot(x=top_reviewers.index, y=top_reviewers.values, ax=axes[1])
axes[1].set_title('Top 10 Reviewers')
axes[1].tick_params(axis='x', rotation=45) # Rotate x-axis labels


# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""#### Dataset profile/user"""

df_user.info()

df_user.describe(include='all')

df_user.groupby('gender').profile.nunique().sort_values(ascending=False)

df_user.gender.isnull().sum()

def check_gender_inconsistency(df):
  """
  Checks for inconsistencies in gender information within a DataFrame.

  Args:
    df: A pandas DataFrame containing a 'profile' column and a 'gender' column.

  Returns:
    A pandas Series with profile names that have inconsistent gender information,
    or None if no inconsistencies are found.
  """

  profiles_with_gender = df.dropna(subset=['gender'])['profile'].unique()
  profiles_without_gender = df[df['gender'].isna()]['profile'].unique()

  inconsistent_profiles = set(profiles_with_gender) & set(profiles_without_gender)

  if inconsistent_profiles:
    return pd.Series(list(inconsistent_profiles), name="Inconsistent Profiles")
  else:
    return None

# Example usage (assuming 'df_user' is your DataFrame):
inconsistent_genders = check_gender_inconsistency(df_user)

if inconsistent_genders is not None:
  print("Profiles with inconsistent gender information:")
  print(inconsistent_genders)
else:
  print("No inconsistencies in gender information found.")

# Create the figure and axes
fig, ax = plt.subplots(figsize=(8, 6))

# Plot the distribution of genders
sns.countplot(x='gender', data=df_user, ax=ax)
ax.set_title('Distribution of User Genders')
ax.set_xlabel('Gender')
ax.set_ylabel('Number of Users')

# Show the plot
plt.show()

#Further visualization for user data

#Distribution of users by birthday year
df_user['birthday'] = pd.to_datetime(df_user['birthday'], errors='coerce')
birthyear = df_user['birthday'].dt.year
plt.figure(figsize=(10,6))
sns.histplot(birthyear.dropna(), kde=True)
plt.title("Distribution of Users' Birth Year")
plt.xlabel('Birth Year')
plt.ylabel('Number of Users')
plt.show()

# Group by genre and get the highest score and most reviews for each genre
genre_stats = df_anime.groupby('genre').agg(
    max_score=('score', 'max'),
    review_count=('uid', 'count')
).reset_index()

# Find the genre with the highest score
highest_score_genre = genre_stats.loc[genre_stats['max_score'].idxmax()]

# Find the genre with the most reviews
most_reviews_genre = genre_stats.loc[genre_stats['review_count'].idxmax()]

print("Genre with the highest score:")
highest_score_genre

print("Genre with the most reviews:")
most_reviews_genre

# Merge anime and review dataframes
merged_df = pd.merge(df_anime, df_review, left_on='uid', right_on='anime_uid', how='inner')

# Group by genre and calculate the average score and number of reviews
genre_performance = merged_df.groupby('genre').agg(
    avg_score=('score_y', 'mean'),
    num_reviews=('uid_y', 'count')
).reset_index()

# Sort by average score and number of reviews to find the most popular genres
genre_performance = genre_performance.sort_values(by=['avg_score', 'num_reviews'], ascending=False)

genre_performance

# Filter genres with average score greater than 8
top_genres = genre_performance[genre_performance['avg_score'] > 8]

# Explode the 'genre' column to separate genres
top_genres['genre'] = top_genres['genre'].str.split(', ')
exploded_genres = top_genres.explode('genre')

# Count the occurrences of each genre
genre_counts = exploded_genres['genre'].value_counts()

# Get the top 10 genres
top_ten_genres = genre_counts.head(10)

top_ten_genres

# Create the bar plot for top_ten_genres
plt.figure(figsize=(10, 6))
sns.barplot(x=top_ten_genres.index, y=top_ten_genres.values)
plt.title('Top 10 Genres')
plt.xlabel('Genre')
plt.ylabel('Number of Animes')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""## Data Preparation"""

# drop duplicate colomn
df_anime.drop_duplicates(inplace=True)
df_review.drop_duplicates(inplace=True)
df_user.drop_duplicates(inplace=True)

#fill missing value on dataset user
df_user['gender'].fillna('Non-Binary', inplace=True)

#fill missing value on birthday with interpolate
df_user['birthday'].fillna(df_user['birthday'].interpolate(), inplace=True)

# add column age based on birthday
df_user['birthday'] = pd.to_datetime(df_user['birthday'])
# Calculate age using days and dividing by 365.25 to approximate years
df_user['age'] = (pd.to_datetime('today') - df_user['birthday']).dt.days / 365.25
df_user['age'] = df_user['age'].astype(int)

# Fill missing values, incrementing the rank for each missing value
next_rank = df_anime['ranked'].max() + 1 if not pd.isna(df_anime['ranked'].max()) else 1

df_anime['ranked'] = df_anime['ranked'].fillna(next_rank + df_anime['ranked'].isnull().cumsum())
df_anime['ranked'] = df_anime['ranked'].astype(int)

# fill missing value on synopsis with mode
df_anime.synopsis.fillna(df_anime.synopsis.mode()[0], inplace=True)

# fill missing value on score with zero
df_anime.score.fillna(0, inplace=True)

# fill missing values episodes to -1
df_anime.episodes.fillna(-1, inplace=True)

df_anime

df_anime.isnull().sum()

df_user

df_review

# prepare dataset for collaborative filtering

df_review_anime = df_review.copy()
df_review_anime.rename(columns={'score':'user_scored'}, inplace=True)
df_review_anime.drop(columns=['text','uid','scores','link'], inplace=True)
df_review_anime = pd.merge(df_review_anime, df_anime[['uid','title','genre','img_url','score']], left_on='anime_uid', right_on='uid', how='left')
df_review_anime

df_review_anime.isnull().sum()

# Create a TF-IDF vectorizer
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(df_anime['genre'])

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
genre_encoded = mlb.fit_transform(df_review_anime['genre'].apply(lambda x: x.replace("[", "").replace("]", "").replace("'", "").split(", ")))

# Create a DataFrame for the encoded genres
genre_df = pd.DataFrame(genre_encoded, columns=mlb.classes_, index=df_review_anime.index)

# Concatenate the encoded genres with the existing DataFrame
df_review_anime = pd.concat([df_review_anime, genre_df], axis=1)

# define rating scale
rating_scale = Reader(rating_scale=(0, 10))

# load data
data = Dataset.load_from_df(df_review_anime[['profile', 'title', 'user_scored',]], rating_scale)

# split dataset
train_set, test_set = surprise_train_test_split(data, test_size=0.20, random_state=42)

# Convert categorical data (profile and title) to numerical representations
user_ids = df_review_anime['profile'].unique()
anime_ids = df_review_anime['title'].unique()

user_to_index = {user: index for index, user in enumerate(user_ids)}
anime_to_index = {anime: index for index, anime in enumerate(anime_ids)}

df_review_anime['user_index'] = df_review_anime['profile'].map(user_to_index)
df_review_anime['anime_index'] = df_review_anime['title'].map(anime_to_index)

# Prepare data for the model
X = df_review_anime[['user_index', 'anime_index']].values
y = df_review_anime['user_scored'].values.astype(float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Modeling

### Content Based
"""

def content_based_recommender(df, title, top_n=10):
    df = df.copy()
    try:
      # Compute cosine similarity matrix
      cosine_sim = cosine_similarity(tfidf_matrix)

      # Get the index of the anime with the given title
      indices = pd.Series(df.index, index=df['title']).drop_duplicates()

      # Function that takes in anime title as input and outputs most similar anime
      idx = indices[title]
      sim_scores = list(enumerate(cosine_sim[idx]))
      sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
      sim_scores = sim_scores[1:top_n+1]  # Exclude the anime itself
      anime_indices = [i[0] for i in sim_scores]

      # Return recommended anime titles with similarity scores
      recommendations = df['title'].iloc[anime_indices].reset_index(drop=True)
      recommendations_df = pd.DataFrame({'title': recommendations, 'Similarity Score': [i[1] for i in sim_scores]})

      return recommendations_df

    except Exception as e:
      print(f"An error occurred: {e}")
      return FileNotFoundError

from fake_useragent import UserAgent

ua = UserAgent()

n=10
top_ten=content_based_recommender(df_anime, 'Naruto', top_n=n)
fig,ax=plt.subplots(1,10,figsize=(17,5))
fig.suptitle("Try these anime",fontsize=40)
if top_ten is None:
    print("No recommendations found.")
else:
  for i in range(len(top_ten["title"].tolist())):
      url=df_anime.loc[df_anime["title"]==top_ten["title"].tolist()[i],"img_url"][:1].values[0]
      try:
          headers = {'User-Agent': ua.random}
          response = requests.get(url, stream=True, headers=headers)
          response.raise_for_status()  # Raise an exception for bad responses (4xx or 5xx)
          img = Image.open(response.raw)
      except requests.exceptions.RequestException as e:
          print(f"Error downloading image from {url}: {e}")
          continue  # Skip to the next image if download fails
      except UnidentifiedImageError as e:
          print(f"Error opening image from {url}: {e}")
          continue  # Skip to the next image if opening fails
      except Exception as e:
          print(f"An unexpected error occurred: {e}")
          continue  # Skip to the next image if any other error occurs
      ax[i].imshow(img)
      ax[i].axis("off")
      ax[i].set_title("Score: {}".format(round(df_anime[df_anime["title"]==top_ten["title"].tolist()[i]]["score"].mean(),1)),y=-0.20,fontsize=10)
      fig.show()

"""### Colaborative filtering

#### SVD
"""

model_svd = SVD()

model_svd.fit(train_set)

predictions_svd = model_svd.test(test_set)

def collaborative_recommender(df, profile, top_n=10):
  all_anime = df['title'].unique()
  rated_anime = df[df['profile'] == profile]['title'].unique()
  unrated_anime = np.setdiff1d(all_anime, rated_anime)
  predictions = [model_svd.predict(profile, anime) for anime in unrated_anime]
  predictions.sort(key=lambda x: x.est, reverse=True)
  top_n_predictions = predictions[:top_n]
  return [(pred.iid, pred.est) for pred in top_n_predictions]

top_ten=collaborative_recommender(df_review_anime, 'Yuez', top_n=10)

fig,ax=plt.subplots(1,10,figsize=(17,5))
fig.suptitle("Try these anime",fontsize=40)
if top_ten is None:
    print("No recommendations found.")
else:
  for i in range(len(top_ten)):
      title = top_ten[i][0]
      url=df_review_anime.loc[df_review_anime["title"]==title,"img_url"][:1].values[0]

      try:
          headers = {'User-Agent': ua.random}
          response = requests.get(url, stream=True, headers=headers)
          response.raise_for_status()  # Raise an exception for bad responses (4xx or 5xx)
          img = Image.open(response.raw)
      except requests.exceptions.RequestException as e:
          print(f"Error downloading image from {url}: {e}")
          continue  # Skip to the next image if download fails
      except UnidentifiedImageError as e:
          print(f"Error opening image from {url}: {e}")
          continue  # Skip to the next image if opening fails
      except Exception as e:
          print(f"An unexpected error occurred: {e}")
          continue  # Skip to the next image if any other error occurs
      ax[i].imshow(img)
      ax[i].axis("off")
      ax[i].set_title("Score: {}".format(round(df_anime[df_anime["title"]==title]["score"].mean(),1)),y=-0.20,fontsize=10)
      fig.show()

"""#### Neural network"""

# Input for user index
user_input = tf.keras.layers.Input(shape=(1,))
user_embedding = tf.keras.layers.Embedding(len(user_ids), 64)(user_input)
user_embedding = tf.keras.layers.Flatten()(user_embedding)

# Input for genre features
genre_input = tf.keras.layers.Input(shape=(genre_encoded.shape[1],))  # Shape based on encoded genre features

# Concatenate user and genre features
merged_features = tf.keras.layers.concatenate([user_embedding, genre_input])

# Hidden layers with regularization and dropout
hidden_layer1 = tf.keras.layers.Dense(512, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.005))(merged_features)
hidden_layer1 = tf.keras.layers.Dropout(0.4)(hidden_layer1)
hidden_layer2 = tf.keras.layers.Dense(256, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01))(hidden_layer1)
hidden_layer2 = tf.keras.layers.Dropout(0.2)(hidden_layer2)  # Add dropout
hidden_layer3 = tf.keras.layers.Dense(128, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01))(hidden_layer2)
hidden_layer3 = tf.keras.layers.Dropout(0.2)(hidden_layer3)  # Add dropout

# Output layer
output = tf.keras.layers.Dense(1)(hidden_layer3)

# Create the model
model = tf.keras.Model(inputs=[user_input, genre_input], outputs=output)

# define callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)

# Compile the model
model.compile(
      optimizer='sgd',
      loss= 'mae',
      metrics= ['mae'],
    )

# Prepare training data
X_train_user = X_train[:, 0].reshape(-1, 1)  # User index
X_train_genre = df_review_anime.loc[X_train_user.flatten(), mlb.classes_.tolist()].values # Genre features for training data

# Fit the model with user and genre inputs
history = model.fit(
    [X_train_user, X_train_genre],  # Input: user index and genre features
    y_train,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping,reduce_learning_rate]
)

# 5. Make recommendations (example)
def neural_network_recommender(user_profile, top_n=10):
    user_idx = user_to_index.get(user_profile)
    if user_idx is None:
        return "User not found"

    unrated_anime_indices = [anime_to_index[anime] for anime in anime_ids if anime not in df_review_anime[df_review_anime['profile']==user_profile]['title'].values]

    # Get genre features for unrated anime
    unrated_anime_genres = df_review_anime.loc[[anime_to_index[anime] for anime in anime_ids if anime not in df_review_anime[df_review_anime['profile']==user_profile]['title'].values] , mlb.classes_.tolist()].values # Genre features for unrated anime

    # Prepare inputs for prediction
    user_input = np.array([user_idx] * len(unrated_anime_indices)).reshape(-1, 1)  # Repeat user_idx for each anime

    # Predict using both user and genre features
    predictions = model.predict([user_input, unrated_anime_genres])

    top_anime_indices = np.argsort(predictions, axis=0)[-top_n:][::-1]  # Get top N indices from sorted array
    top_anime = [anime_ids[idx] for idx in top_anime_indices.flatten()]

    return top_anime

top_10_nn = neural_network_recommender("Yuez", top_n=10)

top_10_nn

"""## Evaluation"""

accuracy.mae(predictions_svd)

accuracy.rmse(predictions_svd)

#For Neural Network
from sklearn.metrics import mean_absolute_error, mean_squared_error
# Prepare test data
X_test_user = X_test[:, 0].reshape(-1, 1)  # User index
X_test_genre = df_review_anime.loc[X_test_user.flatten(), mlb.classes_.tolist()].values  # Genre features for test data

# Make predictions
y_pred = model.predict([X_test_user, X_test_genre])

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"Neural Network MAE: {mae}")
print(f"Neural Network RMSE: {rmse}")

# Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation MAE values
plt.figure(figsize=(10, 5))
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Mean Absolte Error')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()